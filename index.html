<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Learning Long-Context Robot Policies via Past-Token Prediction">
  <meta name="keywords" content="PTP, Long-Context, Robot Learning, Imitation Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Learning Long-Context Robot Policies via Past-Token Prediction</title>

  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <!-- Fonts and Styles -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/stanford.png">

  <!-- Scripts -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    .video-row {
      margin-bottom: 20px;
      padding: 10px;
      border-radius: 8px;
      box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.20);
      background-color: #f9f9f9;
    }
    section.section {
      padding-top: 1rem;
      padding-bottom: 1rem;
    }
    .section h2.title, .section h3.subtitle {
      margin-top: 1.5rem;
      margin-bottom: 1rem;
    }
    .section p {
      margin-top: 0.5rem;
      margin-bottom: 0.5rem;
    }
    .section .title.is-3 {
      text-align: center;
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Learning Long-Context Robot Policies via Past-Token Prediction</h1>
          <div class="is-size-5 publication-authors">
            <!-- Replace these with your actual authors -->
<!--             <span class="author-block">Author 1<sup>*</sup>,</span>
            <span class="author-block">Author 2<sup>*</sup>,</span>
            <span class="author-block">Author 3,</span>
            <span class="author-block">etc.</span>
 -->          </div>
          <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
<!--               <div class="column is-four-fifths">
                <figure class="image is-centered" style="margin-bottom: 0;">
                  <img src="./static/images/logo.jpg" alt="Project Logo">
                </figure>
                <h3 class="title is-4" style="margin-top: 8px;">ICLR 2025</h3>
              </div> -->
            </div>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Add links as needed -->
<!--               <span class="link-block">
                <a href="./static/PTP_paper.pdf" class="external-link button is-normal is-rounded is-dark" target="_blank">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#BibTeX" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-quote-right"></i></span>
                  <span>BibTeX</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Abstract</h2>
    <p>Reasoning over long sequences of observations and actions is essential for many robotic tasks. Yet, learning effective long-context policies from demonstrations remains challenging. As context length increases, training becomes prohibitively expensive due to the surge of memory demands, and policy performance often degrades due to spurious correlations. Recent methods typically sidestep these issues by truncating context length, discarding potentially critical information for subsequent decisions. In this paper, we propose an alternative approach that explicitly regularizes information retention from past observations. At the core of our method is Past-Token Prediction (PTP), an auxiliary task where the policy learns to predict past action tokens alongside future ones. This simple regularizer significantly strengthens temporal action dependencies, which are commonly lost in recent policies. In particular, we find that the benefit of PTP primarily emerges in the policy head rather than the visual encoder. Building on this observation, we introduce a multistage training strategy: first pre-train the visual encoder with short contexts, then fine-tune the policy head using cached long-context embeddings. This approach preserves the benefits of PTP while greatly reducing memory and computational overhead. Beyond training, we further leverage PTP as a self-verification mechanism, enabling the policy to search for action predictions consistent with past actions at test time. Experiments across seven simulated and four real-world tasks demonstrate that our proposed method improves the performance of long-context policies by 3× and accelerates policy training by more than 10×.
    </p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Analysis: Temporal Action Dependency</h2>
    <p>One major challenge in long-context imitation learning is causal confusion, where policies latch onto spurious correlations in the input context that do not truly influence expert behavior. This issue worsens with longer contexts, leading to overfitting during training and poor generalization at deployment. A classic example is copycat behavior, where the model simply mimics past actions without understanding their relationship to observations. However, our findings reveal a different trend: modern policies tend to under-utilize, rather than over-rely on, temporal action dependencies.
    </p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
      <h2 class="title is-3">Method: Past-Token Prediction</h2>
      <p>Building upon our analysis, we introduce a simple yet effective method for long-context imitation learning. At the core is Past-Token Prediction (PTP), an auxiliary objective that tasks the policy to predict both past and future actions. This task encourages the model to better capture action dependencies over time. To scale PTP efficiently, we propose a multi-stage training recipe that freezes a short-horizon encoder, caches visual features, and trains the policy head using these cached embeddings. At test time, we extend PTP into a self-verification mechanism: the policy samples multiple candidate sequences and selects the one that best reconstructs the already-executed actions.
      </p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Results</h2>
    <p>TODO: Add experiment images, video sections, and results here.</p>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{ptp2025,
  title={Learning Long-Context Robot Policies via Past-Token Prediction},
  year={2025},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="content has-text-centered">
    <span class="author-block"><sup>*</sup>Equal Contribution</span>
  </div>
  <div class="container">
    <div class="content has-text-centered">
      <p>Page template adapted from <a href="https://nerfies.github.io/">Nerfies</a>.</p>
    </div>
  </div>
</footer>

</body>
</html>
